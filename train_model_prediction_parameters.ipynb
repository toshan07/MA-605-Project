{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a98753f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T09:19:23.671681Z",
     "iopub.status.busy": "2025-10-13T09:19:23.671374Z",
     "iopub.status.idle": "2025-10-13T09:20:39.825212Z",
     "shell.execute_reply": "2025-10-13T09:20:39.824109Z"
    },
    "papermill": {
     "duration": 76.158983,
     "end_time": "2025-10-13T09:20:39.827075",
     "exception": false,
     "start_time": "2025-10-13T09:19:23.668092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\r\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b98a10",
   "metadata": {},
   "source": [
    "    Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23056f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV = \"/kaggle/input/1st-jan-2015-to-30th-sep-2025-district-data/combined_dataset.csv\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "FEATURE_COLS = [\"tp\",\"sro\",\"ssro\",\"swvl1\",\"swvl2\",\"swvl3\",\"t2m\",\"d2m\"]\n",
    "TIME_COL = \"time\"\n",
    "DIST_COL = \"district\"\n",
    "\n",
    "PAST_SEQ = 168  \n",
    "HORIZON = 24  \n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "EPOCHS = 40\n",
    "K_NEIGHBORS = 4 \n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b558c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_datetime(df, time_col=TIME_COL):\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    return df\n",
    "\n",
    "def date_in_range(ts, start, end):\n",
    "    return (ts >= pd.to_datetime(start)) & (ts <= pd.to_datetime(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac39e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T09:20:39.872934Z",
     "iopub.status.busy": "2025-10-13T09:20:39.872658Z",
     "iopub.status.idle": "2025-10-13T10:00:12.443666Z",
     "shell.execute_reply": "2025-10-13T10:00:12.442427Z"
    },
    "papermill": {
     "duration": 2372.595001,
     "end_time": "2025-10-13T10:00:12.445063",
     "exception": true,
     "start_time": "2025-10-13T09:20:39.850062",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CSV: /kaggle/input/1st-jan-2015-to-30th-sep-2025-district-data/combined_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/3502595890.py:79: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  min_time = df[TIME_COL].min().floor('H')\n",
      "/tmp/ipykernel_18/3502595890.py:80: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  max_time = df[TIME_COL].max().ceil('H')\n",
      "/tmp/ipykernel_18/3502595890.py:83: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_time_index = pd.date_range(start=min_time, end=max_time, freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1121748 unique districts: 12\n",
      "num_nodes: 12\n",
      "Time range: 2015-01-01 00:00:00 to 2025-09-30 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building district panels:   0%|          | 0/12 [00:00<?, ?it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:   8%|▊         | 1/12 [00:00<00:01,  6.38it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  17%|█▋        | 2/12 [00:00<00:01,  6.70it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  25%|██▌       | 3/12 [00:00<00:01,  6.64it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  33%|███▎      | 4/12 [00:00<00:01,  6.70it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  42%|████▏     | 5/12 [00:00<00:01,  6.86it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  50%|█████     | 6/12 [00:00<00:00,  6.87it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  58%|█████▊    | 7/12 [00:01<00:00,  6.88it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  67%|██████▋   | 8/12 [00:01<00:00,  6.95it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  75%|███████▌  | 9/12 [00:01<00:00,  7.00it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  83%|████████▎ | 10/12 [00:01<00:00,  6.97it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels:  92%|█████████▏| 11/12 [00:01<00:00,  6.96it/s]/tmp/ipykernel_18/3502595890.py:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
      "Building district panels: 100%|██████████| 12/12 [00:01<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data array shape: (12, 94224, 8)\n",
      "Train hours: 92760 Val hours: 1464\n",
      "Fitting scaler on shape: (1113120, 8)\n",
      "Building adjacency matrix via correlation-based kNN (K= 4 )\n",
      "Adjacency built. Sample row sums (should be ~1): [0.9999999  0.99999976 0.9999998  0.9999998  0.9999995 ]\n",
      "Built dataset with 92569 windows (past=168, horizon=24).\n",
      "Built dataset with 1441 windows (past=168, horizon=24).\n",
      "STGNN(\n",
      "  (input_proj): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (temporal1): TemporalConvBlock(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (act): ReLU()\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (temporal2): TemporalConvBlock(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "    (act): ReLU()\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (spatial_fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (forecast): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=192, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 137616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss=0.195922 val_loss=0.407362\n",
      "Saved best model to outputs/stgnn_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss=0.127958 val_loss=0.429522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss=0.118408 val_loss=0.342044\n",
      "Saved best model to outputs/stgnn_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss=0.110275 val_loss=0.369101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss=0.109263 val_loss=0.350076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss=0.103938 val_loss=0.360470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss=0.098446 val_loss=0.335789\n",
      "Saved best model to outputs/stgnn_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss=0.094213 val_loss=0.378294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss=0.090818 val_loss=0.368243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss=0.085929 val_loss=0.356508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train_loss=0.083979 val_loss=0.398043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train_loss=0.082549 val_loss=0.363089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train_loss=0.080480 val_loss=0.386498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train_loss=0.079530 val_loss=0.371296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 train_loss=0.078752 val_loss=0.359304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 train_loss=0.077610 val_loss=0.342389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 train_loss=0.077482 val_loss=0.419405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 train_loss=0.076253 val_loss=0.361707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 train_loss=0.075326 val_loss=0.354129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 train_loss=0.074080 val_loss=0.346620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 train_loss=0.074933 val_loss=0.366860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 train_loss=0.072245 val_loss=0.414963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 train_loss=0.073133 val_loss=0.379092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 train_loss=0.073105 val_loss=0.400041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 train_loss=0.071747 val_loss=0.366948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 train_loss=0.071244 val_loss=0.369868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 train_loss=0.071320 val_loss=0.344993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 train_loss=0.070853 val_loss=0.396829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 train_loss=0.070915 val_loss=0.367284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 train_loss=0.071066 val_loss=0.361225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 train_loss=0.070456 val_loss=0.376459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 train_loss=0.069392 val_loss=0.365527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 train_loss=0.069586 val_loss=0.373855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 train_loss=0.069844 val_loss=0.379926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 train_loss=0.070688 val_loss=0.366710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 train_loss=0.069211 val_loss=0.372662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 train_loss=0.068726 val_loss=0.391472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 train_loss=0.068942 val_loss=0.392157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 train_loss=0.068487 val_loss=0.380365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 train_loss=0.067633 val_loss=0.408765\n",
      "Running detailed validation visualization (using best model)\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18/3502595890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running detailed validation visualization (using best model)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "print(\"Loading CSV:\", DATA_CSV)\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "df = ensure_datetime(df, TIME_COL)\n",
    "use_cols = [DIST_COL, TIME_COL] + FEATURE_COLS\n",
    "df = df[use_cols].drop_duplicates(subset=[DIST_COL, TIME_COL])\n",
    "print(\"Loaded rows:\", len(df), \"unique districts:\", df[DIST_COL].nunique())\n",
    "\n",
    "districts = sorted(df[DIST_COL].unique())\n",
    "district_to_idx = {d:i for i,d in enumerate(districts)}\n",
    "num_nodes = len(districts)\n",
    "print(\"num_nodes:\", num_nodes)\n",
    "\n",
    "min_time = df[TIME_COL].min().floor('H')\n",
    "max_time = df[TIME_COL].max().ceil('H')\n",
    "print(\"Time range:\", min_time, \"to\", max_time)\n",
    "\n",
    "full_time_index = pd.date_range(start=min_time, end=max_time, freq='H')\n",
    "\n",
    "panel = {}\n",
    "for d in tqdm(districts, desc=\"Building district panels\"):\n",
    "    sub = df[df[DIST_COL]==d].set_index(TIME_COL).reindex(full_time_index)\n",
    "    sub_interp = sub[FEATURE_COLS].astype(float).interpolate(method='time', limit_direction='both')\n",
    "    sub_interp = sub_interp.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "    panel[d] = sub_interp\n",
    "\n",
    "T = len(full_time_index)\n",
    "F = len(FEATURE_COLS)\n",
    "data_array = np.zeros((num_nodes, T, F), dtype=np.float32)\n",
    "for i,d in enumerate(districts):\n",
    "    data_array[i] = panel[d].values\n",
    "\n",
    "print(\"Data array shape:\", data_array.shape)  \n",
    "train_end = pd.to_datetime(\"2025-07-31 23:00:00\")\n",
    "val_start = pd.to_datetime(\"2025-08-01 00:00:00\")\n",
    "val_end = pd.to_datetime(\"2025-09-30 23:00:00\")\n",
    "\n",
    "train_mask = (full_time_index <= train_end)\n",
    "val_mask = (full_time_index >= val_start) & (full_time_index <= val_end)\n",
    "print(\"Train hours:\", train_mask.sum(), \"Val hours:\", val_mask.sum())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_for_scaler = data_array[:, train_mask, :].reshape(-1, F)\n",
    "print(\"Fitting scaler on shape:\", train_data_for_scaler.shape)\n",
    "scaler.fit(train_data_for_scaler)\n",
    "# apply\n",
    "data_scaled = np.zeros_like(data_array)\n",
    "for i in range(num_nodes):\n",
    "    data_scaled[i] = scaler.transform(data_array[i])\n",
    "\n",
    "print(\"Building adjacency matrix via correlation-based kNN (K=\", K_NEIGHBORS, \")\")\n",
    "agg_series = data_array.mean(axis=2)\n",
    "agg_train = agg_series[:, train_mask]\n",
    "\n",
    "corr = np.corrcoef(agg_train) \n",
    "corr = np.nan_to_num(corr)\n",
    "A = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "for i in range(num_nodes):\n",
    "    neighbors = np.argsort(-corr[i]) \n",
    "    cnt = 0\n",
    "    for j in neighbors:\n",
    "        if i==j: continue\n",
    "        A[i,j] = max(0.0, corr[i,j])\n",
    "        cnt += 1\n",
    "        if cnt >= K_NEIGHBORS: break\n",
    "A = (A + A.T) / 2.0\n",
    "row_sum = A.sum(axis=1, keepdims=True) + 1e-6\n",
    "A = A / row_sum\n",
    "\n",
    "print(\"Adjacency built. Sample row sums (should be ~1):\", A.sum(axis=1)[:5])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(A, cmap=\"viridis\")\n",
    "plt.title(\"Adjacency matrix (correlation kNN)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"adjacency_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "class STDataset(Dataset):\n",
    "    def __init__(self, data_scaled, time_index, mask_hours, past_seq=PAST_SEQ, horizon=HORIZON, nodes_first=True):\n",
    "        self.data = data_scaled\n",
    "        self.time_index = time_index\n",
    "        self.mask = mask_hours\n",
    "        self.nodes = data_scaled.shape[0]\n",
    "        self.T = data_scaled.shape[1]\n",
    "        self.F = data_scaled.shape[2]\n",
    "        self.past = past_seq\n",
    "        self.horizon = horizon\n",
    "        self.indices = []\n",
    "        for t in range(self.past, self.T - self.horizon + 1):\n",
    "            input_mask = True\n",
    "            if not self.mask[t] and not all(self.mask[t-self.past:t]): \n",
    "                continue\n",
    "            if not all(self.mask[t:t+self.horizon]):\n",
    "                continue\n",
    "            self.indices.append(t)\n",
    "        print(f\"Built dataset with {len(self.indices)} windows (past={self.past}, horizon={self.horizon}).\")\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.indices[idx]\n",
    "        x = self.data[:, t-self.past:t, :]   \n",
    "        y = self.data[:, t:t+self.horizon, :] \n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), t\n",
    "\n",
    "train_mask_bool = np.array(train_mask, dtype=bool)\n",
    "val_mask_bool = np.array(val_mask, dtype=bool)\n",
    "\n",
    "train_ds = STDataset(data_scaled, full_time_index, train_mask_bool, past_seq=PAST_SEQ, horizon=HORIZON)\n",
    "val_ds = STDataset(data_scaled, full_time_index, val_mask_bool, past_seq=PAST_SEQ, horizon=HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "class TemporalConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size, padding=(kernel_size-1)//2 * dilation, dilation=dilation)\n",
    "        self.act = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "    def forward(self, x): \n",
    "        return self.bn(self.act(self.conv(x)))\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, num_nodes, in_feats, hidden_feats=64, horizon=HORIZON, A_init=None):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_feats = in_feats\n",
    "        self.horizon = horizon\n",
    "\n",
    "        if A_init is None:\n",
    "            A_init = np.eye(num_nodes, dtype=np.float32)\n",
    "        A_tensor = torch.tensor(A_init, dtype=torch.float32)\n",
    "        self.register_buffer(\"A_init\", A_tensor)\n",
    "        self.A_weight = nn.Parameter(torch.ones_like(A_tensor) * 0.1) \n",
    "        self.input_proj = nn.Linear(in_feats, hidden_feats)\n",
    "        self.temporal1 = TemporalConvBlock(hidden_feats, hidden_feats, kernel_size=3)\n",
    "        self.temporal2 = TemporalConvBlock(hidden_feats, hidden_feats, kernel_size=3, dilation=2)\n",
    "        self.spatial_fc = nn.Linear(hidden_feats, hidden_feats)\n",
    "        self.forecast = nn.Sequential(\n",
    "            nn.Linear(hidden_feats, hidden_feats//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feats//2, horizon * in_feats)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, T_in, F = x.shape\n",
    "        x_proj = self.input_proj(x) \n",
    "        hidden = x_proj.permute(0,1,3,2).contiguous()  \n",
    "        B,N,H,T = hidden.shape\n",
    "        hidden_reshape = hidden.view(B*N, H, T)\n",
    "        hidden_t = self.temporal1(hidden_reshape)\n",
    "        hidden_t = self.temporal2(hidden_t) \n",
    "        hidden_t = hidden_t.view(B, N, H, T)\n",
    "        node_repr = hidden_t.mean(dim=-1)\n",
    "        A_eff = self.A_init * self.A_weight \n",
    "        row_sum = A_eff.sum(dim=1, keepdim=True) + 1e-6\n",
    "        A_norm = A_eff / row_sum\n",
    "        agg = torch.einsum(\"nm, bmh -> bnh\", A_norm, node_repr)\n",
    "        agg = torch.relu(self.spatial_fc(agg))\n",
    "        out = self.forecast(agg) \n",
    "        out = out.view(B, N, self.horizon, F) \n",
    "        return out\n",
    "\n",
    "model = STGNN(num_nodes=num_nodes, in_feats=F, hidden_feats=128, horizon=HORIZON, A_init=A).to(DEVICE)\n",
    "print(model)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val = 1e18\n",
    "save_model_path = os.path.join(OUTPUT_DIR, \"stgnn_best.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} (train)\", leave=False)\n",
    "    for xb, yb, t_idxs in pbar:\n",
    "        xb = xb.to(DEVICE)   \n",
    "        yb = yb.to(DEVICE)   \n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(xb)    \n",
    "        loss = criterion(yhat, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    epoch_train_loss = running_loss / len(train_ds)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    all_y = []\n",
    "    all_yhat = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, t_idxs in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            val_running += loss.item() * xb.size(0)\n",
    "            all_y.append(yb.cpu().numpy())\n",
    "            all_yhat.append(yhat.cpu().numpy())\n",
    "    epoch_val_loss = val_running / len(val_ds) if len(val_ds)>0 else np.nan\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch} train_loss={epoch_train_loss:.6f} val_loss={epoch_val_loss:.6f}\")\n",
    "\n",
    "    if epoch_val_loss < best_val:\n",
    "        best_val = epoch_val_loss\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"scaler\": scaler, \"config\": {\"past\":PAST_SEQ, \"horizon\":HORIZON, \"features\": FEATURE_COLS, \"districts\":districts}},\n",
    "                   save_model_path)\n",
    "        print(\"Saved best model to\", save_model_path)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.plot(val_losses, label=\"val_loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"MSE loss\"); plt.legend(); plt.title(\"Training curves\")\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Running detailed validation visualization (using best model)\")\n",
    "ckpt = torch.load(save_model_path, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "N_plot_batches = 6\n",
    "val_iter = iter(val_loader)\n",
    "sample_count = 0\n",
    "all_preds = []\n",
    "all_trues = []\n",
    "sample_meta = []\n",
    "with torch.no_grad():\n",
    "    for bidx in range(N_plot_batches):\n",
    "        try:\n",
    "            xb, yb, t_idxs = next(val_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "        yhat = model(xb)\n",
    "        all_preds.append(yhat.cpu().numpy()) \n",
    "        all_trues.append(yb.cpu().numpy())\n",
    "        sample_meta.append(t_idxs.numpy())\n",
    "        sample_count += xb.size(0)\n",
    "\n",
    "if len(all_preds)==0:\n",
    "    print(\"No validation windows found for plotting.\")\n",
    "else:\n",
    "    preds = np.concatenate(all_preds, axis=0)[:100] \n",
    "    trues = np.concatenate(all_trues, axis=0)[:100]\n",
    "    S = preds.shape[0]\n",
    "    preds_reshaped = preds.reshape(-1, F)\n",
    "    trues_reshaped = trues.reshape(-1, F)\n",
    "    preds_inv = scaler.inverse_transform(preds_reshaped).reshape(S, num_nodes, HORIZON, F)\n",
    "    trues_inv = scaler.inverse_transform(trues_reshaped).reshape(S, num_nodes, HORIZON, F)\n",
    "    metrics = {}\n",
    "    for fi, fname in enumerate(FEATURE_COLS):\n",
    "        y_true_f = trues_inv[:,:,:,fi].ravel()\n",
    "        y_pred_f = preds_inv[:,:,:,fi].ravel()\n",
    "        mse = mean_squared_error(y_true_f, y_pred_f)\n",
    "        metrics[fname] = {\"mse\": mse}\n",
    "    print(\"Validation metrics (sampled windows):\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"  {k}: MSE={v['mse']:.4f}\")\n",
    "    plot_district_indices = [0, max(0,num_nodes//3), max(0, 2*num_nodes//3), num_nodes-1]\n",
    "    for d_idx in plot_district_indices:\n",
    "        for fi, fname in enumerate(FEATURE_COLS):\n",
    "            plt.figure(figsize=(10,4))\n",
    "            s = 0\n",
    "            t = 0\n",
    "            true_ts = trues_inv[s, d_idx, :, fi]\n",
    "            pred_ts = preds_inv[s, d_idx, :, fi]\n",
    "            hours = np.arange(HORIZON)\n",
    "            plt.plot(hours, true_ts, label=\"true\", marker='o')\n",
    "            plt.plot(hours, pred_ts, label=\"pred\", marker='x')\n",
    "            plt.title(f\"District={districts[d_idx]} | feature={fname} | horizon={HORIZON}h\")\n",
    "            plt.xlabel(\"hours ahead\"); plt.ylabel(fname)\n",
    "            plt.legend()\n",
    "            outfn = os.path.join(OUTPUT_DIR, f\"ts_d{d_idx}_{districts[d_idx]}_{fname}.png\")\n",
    "            plt.savefig(outfn); plt.close()\n",
    "    for fi, fname in enumerate(FEATURE_COLS):\n",
    "        y_true_f = trues_inv[:,:,:,fi].ravel()\n",
    "        y_pred_f = preds_inv[:,:,:,fi].ravel()\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.scatterplot(x=y_true_f, y=y_pred_f, s=10, alpha=0.3)\n",
    "        plt.plot([y_true_f.min(), y_true_f.max()], [y_true_f.min(), y_true_f.max()], 'r--')\n",
    "        plt.xlabel(\"true\"); plt.ylabel(\"pred\")\n",
    "        plt.title(f\"Scatter: {fname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"scatter_{fname}.png\"))\n",
    "        plt.close()\n",
    "    node_errors = []\n",
    "    for n in range(num_nodes):\n",
    "        y_t = trues_inv[:,:, :, :].reshape(-1, F)[:, :]\n",
    "    per_node_rmse = []\n",
    "    for n in range(num_nodes):\n",
    "        y_true_n = trues_inv[:, n, :, :].reshape(-1, F)\n",
    "        y_pred_n = preds_inv[:, n, :, :].reshape(-1, F)\n",
    "        rmse = np.sqrt(((y_true_n - y_pred_n)**2).mean())\n",
    "        per_node_rmse.append(rmse)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sns.barplot(x=districts, y=per_node_rmse)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Per-district RMSE (sampled)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"per_district_rmse.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    for fi,fname in enumerate(FEATURE_COLS):\n",
    "        res = (trues_inv[:,:,:,fi] - preds_inv[:,:,:,fi]).ravel()\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.histplot(res, bins=80, kde=True)\n",
    "        plt.title(f\"Residuals histogram: {fname}\")\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"resid_hist_{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "print(\"All plots saved to\", OUTPUT_DIR)\n",
    "print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9a503",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8478133,
     "sourceId": 13365176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2461.494448,
   "end_time": "2025-10-13T10:00:21.440264",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-13T09:19:19.945816",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
